{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd01c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\" \", sep=\",\")\n",
    "\n",
    "# Extract the 'content' column\n",
    "a = df[\"content\"]\n",
    "\n",
    "# Cleaning the text\n",
    "a = a.str.replace(r'[^\\x01-\\x7F]', '', regex=True)  # Remove non-ASCII characters\n",
    "a = a.str.replace(r'http\\S+\\s*', '', regex=True)     # Remove URLs\n",
    "a = a.str.replace(r'\\bRT\\b', '', regex=True)         # Remove 'RT'\n",
    "a = a.str.replace(r'#', '', regex=True)              # Remove hashtags\n",
    "a = a.str.replace(r'@\\S+', '', regex=True)           # Remove mentions\n",
    "a = a.str.replace(r'[\\x00-\\x1F\\x7F]', '', regex=True) # Remove control characters\n",
    "a = a.str.replace(r'\\d', '', regex=True)             # Remove digits\n",
    "a = a.str.replace(r'[^\\w\\s]', '', regex=True)        # Remove punctuation\n",
    "a = a.str.replace(r'^\\s*', '', regex=True)           # Remove leading whitespace\n",
    "a = a.str.replace(r'\\s*$', '', regex=True)           # Remove trailing whitespace\n",
    "\n",
    "# Convert to lowercase\n",
    "a = a.str.lower()\n",
    "\n",
    "# Define specific stopwords\n",
    "custom_stopwords = [\"covid-19 vaccine\", \"covid\", \"vaccine\", \"vaccines\", \"coronavirus\", \"covidvaccine\"]\n",
    "\n",
    "# Remove specific stopwords\n",
    "def remove_custom_stopwords(text):\n",
    "    return ' '.join(word for word in text.split() if word not in custom_stopwords)\n",
    "\n",
    "#a = a.apply(remove_custom_stopwords)\n",
    "\n",
    "# Create a new DataFrame with the cleaned tweets\n",
    "df['cleaned_content'] = a.apply(remove_custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae9f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "import gensim\n",
    "\n",
    "# Additional preprocessing of the cleaned tweets\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "data = df.cleaned_content.values.tolist()\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ee0db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords and lemmatize the text\n",
    "import nltk \n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "nltk.download('stopwords')  \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import spacy \n",
    "#def remove_stopwords(texts):\n",
    " #   return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags and len(token)>3] )\n",
    "    return texts_out\n",
    "#data_words_nostops = remove_stopwords(data_words)\n",
    "#data_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']) \n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']) \n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807969b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove commas and merge each inner list into a single string\n",
    "cleaned_data = [\" \".join(item).replace(\",\", \"\") for item in data_lemmatized]\n",
    "\n",
    "# Convert the list of cleaned strings to a DataFrame\n",
    "df[\"lemmatized_text\"] = cleaned_data\n",
    "df = df[df[\"lemmatized_text\"] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837bbc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the 'date' column is in datetime format\n",
    "df[\"date_e\"] = pd.to_datetime(df[\"date\"])\n",
    "# Extract the month number\n",
    "df[\"month\"] = df[\"date_e\"].dt.month\n",
    "# If you want the month name instead of the number, use:\n",
    "df[\"month_name\"] = df[\"date_e\"].dt.strftime(\"%B\")\n",
    "# Display the updated DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45334a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.lemmatized_text.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "#Create TF-IDF representation of the tweets\n",
    "from gensim import corpora, models\n",
    "dictionary = gensim.corpora.Dictionary(data_words)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in data_words]\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97031af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#Generate Coherence plot to find the optimal number of topics\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model=gensim.models.LdaMulticore(corpus_tfidf, num_topics=num_topics, id2word=dictionary)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=data_words, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus_tfidf, texts=data_words, start=2, limit=40, step=6)\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64152b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import ldaseqmodel\n",
    "from gensim.corpora import Dictionary, bleicorpus\n",
    "import numpy\n",
    "from gensim.matutils import hellinger\n",
    "month_counts = df[\"month_name\"].value_counts().sort_index()\n",
    "dec=month_counts[0]\n",
    "jan=month_counts[1]\n",
    "feb=month_counts[2]\n",
    "# Create time slices based on the number of tweets in each month. Replace the values with number of records for each time slice in your dataset\n",
    "time_slice = [dec,jan,feb]\n",
    "\n",
    "# Train the LDA model with the optimal number of topics. Replace 'num_topics=8' with the number of optimal topics identified in your dataset\n",
    "ldaseq = ldaseqmodel.LdaSeqModel(corpus=corpus_tfidf, id2word=dictionary, time_slice=time_slice, num_topics=8, chain_variance=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649b7d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first topic for all three time slices. Change 'topic=0' with any other topic number to print that topic\n",
    "ldaseq.print_topic_times(topic=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fb12e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all the topic for first time slice. Change 'time=0' with any other time slice number to print topics for that time slice\n",
    "ldaseq.print_topics(time=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11bc77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaSeqModel\n",
    "\n",
    "# Save the LDA model\n",
    "ldaseq.save('ldaseq')\n",
    "\n",
    "# If we've saved before simply load the model\n",
    "dtm_model = LdaSeqModel.load('ldaseq')\n",
    "topics_dtm = dtm_model.print_topic_times(topic=0)\n",
    "topics_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d072003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify topic number for each tweet\n",
    "num_rows = len(bow_corpus)\n",
    "max_index=[]\n",
    "for j in range(num_rows):\n",
    "    doc = dtm_model.doc_topics(j)\n",
    "    doc_lis=doc.tolist()\n",
    "    max_value = max(doc_lis) \n",
    "    max_index.append(doc_lis.index(max_value))\n",
    "    \n",
    "df3 = pd.DataFrame(max_index, columns=[\"topic_number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883dd662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df and df3\n",
    "df = pd.concat([df, df3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8b4074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution of each topic for each time slice. Replace 'time=0' with the time sloce for which you need to generate visualization of the topics\n",
    "import pyLDAvis\n",
    "doc_topic, topic_term, doc_lengths, term_frequency, vocab = ldaseq.dtm_vis(time=0, corpus=bow_corpus)\n",
    "vis_wrapper = pyLDAvis.prepare(topic_term_dists=topic_term, doc_topic_dists=doc_topic, doc_lengths=doc_lengths, vocab=vocab, term_frequency=term_frequency)\n",
    "pyLDAvis.display(vis_wrapper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
